<!DOCTYPE html>

<html lang="en">

    <head>
    	<title>Curtis Thompson - PetFinder: First Entry</title>
    	<link rel="stylesheet" type="text/css" href="/stylesheets/main.css" />
    	<link rel="stylesheet" type="text/css" href="/stylesheets/notes.css" />
    	<meta name="author" content="Curtis Thompson" />
    	<meta name="description" content="During my previous term at university I took a module in machine learning. It was my first actual experience with machine learning, so I found it both challenging and exciting. The second coursework for the module was to participate in a Kaggle competition: PLAsTiCC Astronomical Classification." />
    	<meta name="keywords" content="curtis, thompson, kaggle, cats, dogs, animals, PetFinder, competition, machine learning, classification, LightGBM" />
		<?php include($_SERVER['DOCUMENT_ROOT'] . '/php-assets/core-header.php'); ?>
    </head>
    
    
    <body>
		<?php include($_SERVER['DOCUMENT_ROOT'] . '/php-assets/menu.php'); ?>
	
		<div id="main-content">
    		<h1 id="small-header-box">PetFinder: First Entry</h1>
    		<div class="notes-text">
        		<div class="notes-details"><span class="author">Curtis Thompson</span><span class="date">Thursday 3rd January 2019</span><span class="topic">Artificial Intelligence</span></div>		
        		<p>During my previous term at university I took a module in machine learning. It was my first actual experience with machine learning, so I found it both challenging and exciting. The second coursework for the module was to participate in a Kaggle competition: PLAsTiCC Astronomical Classification. In the competition we were given astronomical data collected from telescopes, on over three million different objects, and were expected to predict the type of class that each of these objects belonged to (from 15 different unknown astronomical classes). It was not like anything I had ever done before, and with just a few weeks of machine learning practice I was able to place 792nd out of 1094 participants on Kaggle. At the end of the competition I had 9 weeks of experience with machine learning, so I was proud of my position, but I felt like I could do much better. It is for this reason that I have entered a second Kaggle competition: PetFinder.my Adoption Prediction. These notes, and hopefully the many more to come, will document my experiences in this Kaggle competition.</p>
                <p>I have now been a participant for approximately a week. I am currently positioned 24th on the leaderboard out of 325 participants (top 8%) with a leaderboard score of 0.399 (scored by a Quadratic Weighted Kappa). This is a very good position so far but is largely based on other participants' kernels and discussions.</p>
                <p>My initial idea was to read in all the data from the competition and then feed it into a random forest classifier. The second version of this code scored 0.306. This would give me a basic benchmark to aim for with future models. I also tried an MLP classifier as it was my best model in the previous competition but after scaling data it did not perform as well as the random forest classifier. I considered a few further features that could be extracted from the text data (pet descriptions) given to us such as description length, amount of punctuation, or amount of different parts-of-speech, and implemented a few of these quickly. </p>
                <p>However, since that I have spent most of my time exploring the data given to us and looking through ideas from other participants. One kernel that I <a href="https://www.kaggle.com/tunguz/annoying-ab-shreck-and-bluetooth">forked from Bojan Tunguz</a> (most of the work in this <a href="https://www.kaggle.com/abhishek/maybe-something-interesting-here">kernel is actually from Abhishek</a>) was a LightGBM model that scored me 0.399 on the leaderboard. I have seen LightGBM mentioned before in my previous competition, but I had no experience with it myself, so played around with it a little. All tweaks I made to the kernel decreased the leaderboard score, despite many of them having a slightly higher cross-validation score. These tweaks included a few hyper-parameters and features. For example, the introduction of the text features I previously mentioned decreases the leaderboard score to 0.389. Perhaps if my knowledge of LightGBM was greater I would be able to improve the score, but I also know that I was need better features for my model. Both of these are areas that I need to look into for this competition.</p>
                <p>I have two main kernels that are currently private. One is my LightGBM kernel, and the other is a data exploration kernel (there is also actually some random forest code, but I have not focused on it). For now, that is all I have done. In the next few days I am hoping to find a better way of getting useful features for my model, and I will do some reading on LightGBM. That's all for now.</p>
        	</div>
            
            <div class="notes-others">
        		<?php include($_SERVER['DOCUMENT_ROOT'] . '/php-assets/other-notes.php'); ?>
            </div>
    	</div>
		
		<?php include($_SERVER['DOCUMENT_ROOT'] . '/php-assets/footer.php'); ?>
    </body>

</html>